{"nbformat_minor": 0, "cells": [{"source": "#Spark Demo", "cell_type": "markdown", "metadata": {}}, {"source": "Start the kernel", "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": "nitro.spark.create_kernel(\"http://169.53.153.8:9000/\")", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Load data from the file", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "val iris = sc.textFile(\"hdfs://10.122.48.12:8020/data/iris.csv\")", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Count elements", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "iris.count()", "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "150"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "\r                                                                                \r"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Create a random sample Array (no replacement, 10 elements)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "iris.takeSample(false,10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "\r                                                                                \r"}, {"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "Array(5.0,3.4,1.6,0.4,setosa, 5.1,3.3,1.7,0.5,setosa, 5.7,2.6,3.5,1.0,versicolor, 5.3,3.7,1.5,0.2,setosa, 6.0,2.9,4.5,1.5,versicolor, 5.5,2.5,4.0,1.3,versicolor, 6.5,3.0,5.5,1.8,virginica, 5.4,3.7,1.5,0.2,setosa, 6.3,3.3,4.7,1.6,versicolor, 6.4,3.1,5.5,1.8,virginica)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Read the first line of the RDD", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "iris.first()", "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "5.1,3.5,1.4,0.2,setosa"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Filter lines that contain a specified word.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 6, "cell_type": "code", "source": "val versicolorLines = iris.filter(line => line.contains(\"versicolor\"))\nversicolorLines.first()", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "7.0,3.2,4.7,1.4,versicolor"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Count the words in the file", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "//grab iris and split the lines into words\nval words = iris.flatMap(line => line. split(\",\")) \nprintln(words.first()) //first line is 5.1,3.5,1.4,0.2,setosa\n// Transform into pairs and count.\nval counts = words.map(word => (word, 1)).reduceByKey{case (x, y) => x + y}\nprintln(counts.count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "5.1\n77\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Get only unique members of the RDD", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "val dist = iris.distinct()\ndist.count()", "outputs": [{"execution_count": 11, "output_type": "execute_result", "data": {"text/plain": "147"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "#Mlllib \nMachine learning with Spark", "cell_type": "markdown", "metadata": {}}, {"source": "Compute Summary Statistics", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n\n//load and parse data to vectors\nval iris = sc.textFile(\"hdfs://10.122.48.12:8020/data/iris.csv\")\nval data_iris = iris.map(l => l.split(\",\",-1) )\nval parsedData = data_iris.map(r => Vectors.dense(Array( r(0).toDouble,\nr(1).toDouble, r(2).toDouble, r(3).toDouble))).cache()\n\n// Compute column summary statistics.\nval summary: MultivariateStatisticalSummary = Statistics.colStats(parsedData)\n\nprintln(\"Mean: \" + summary.mean) // a dense vector containing the mean value for each column\nprintln(\"Variance: \"+ summary.variance) // column-wise variance\nprintln(\"Min: \"+ summary.min) // minimum value of each column\nprintln(\"Max: \"+ summary.max) // maximum value of each column\nprintln(\"L1norm: \"+ summary.normL1) // L1 norm of each column\nprintln(\"L2norm: \"+ summary.normL2) // Euclidean magnitude of each column", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Mean: [5.843333333333332,3.0540000000000003,3.7586666666666666,1.1986666666666668]\nVariance: [0.6856935123042509,0.18800402684563744,3.113179418344516,0.5824143176733783]\nMin: [4.3,2.0,1.0,0.1]\nMax: [7.9,4.4,6.9,2.5]\nL1norm: [876.4999999999998,458.1000000000001,563.8000000000002,179.79999999999995]\nL2norm: [72.27620631992245,37.77631533117014,50.82322303829225,17.38677658451963]\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##Train a NaiveBayes Classifier", "cell_type": "markdown", "metadata": {}}, {"source": "MLlib supports multinomial naive Bayes, which is typically used for document classification. Within that context, each observation is a document and each feature represents a term whose value is the frequency of the term. Feature values must be nonnegative to represent term frequencies.  \n\nParse data and generate random id for Iris objects", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "import org.apache.spark.mllib.classification.NaiveBayes\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nval iris = sc.textFile(\"hdfs://10.122.48.12:8020/data/iris.csv\")\nval parsedData = iris.map { line =>\n  val parts = line.split(',')\n  LabeledPoint(parts(0).toDouble, Vectors.dense(parts.dropRight(1).map(_.toDouble)))\n}\nparsedData.take(2).foreach(println)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(5.1,[5.1,3.5,1.4,0.2])\n(4.9,[4.9,3.0,1.4,0.2])\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "NaiveBayes implements multinomial naive Bayes. It takes an RDD of LabeledPoint and an optional smoothing parameter lambda as input, and output a NaiveBayesModel, which can be used for evaluation and prediction.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "// Split data into training (40%) and test (60%).\nval splits = parsedData.randomSplit(Array(0.4, 0.6), seed = 11L)\nval training = splits(0)\nval test = splits(1)\n\nval model = NaiveBayes.train(training, lambda = 1.0)\n\nval predictionAndLabel = test.map(p => (model.predict(p.features), p.label))\nval accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()\n//comparing the model's score to the model's predicted class \nprintln(accuracy)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.06382978723404255\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##K-means Clustering", "cell_type": "markdown", "metadata": {}}, {"execution_count": 42, "cell_type": "code", "source": "import org.apache.spark.mllib.clustering.KMeans\nimport org.apache.spark.mllib.linalg.Vectors\n\n//load and parse data to vectors\nval iris = sc.textFile(\"hdfs://10.122.48.12:8020/data/iris.csv\")\nval data_iris = iris.map(l => l.split(\",\",-1) )\nval parsedData = data_iris.map(r => Vectors.dense(Array( r(0).toDouble, \nr(1).toDouble, r(2).toDouble, r(3).toDouble))).cache()\n// Cluster the data into three classes using KMeans.\nval numClusters = 3\nval numIterations = 20\nval clusters = KMeans.train(parsedData, numClusters, numIterations)\n// Compute the sum of squared errors.\nval cost = clusters.computeCost(parsedData)\nprintln(\"Sum of squared errors = \" + cost)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Sum of squared errors = 78.94084142614622\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##PCA", "cell_type": "markdown", "metadata": {}}, {"execution_count": 48, "cell_type": "code", "source": "import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n\nval iris = sc.textFile(\"hdfs://10.122.48.12:8020/data/iris.csv\")\n//parse csv to vectors\nval data_iris = iris.map(l => l.split(\",\",-1) )\nval parsedData = data_iris.map(r => Vectors.dense(Array( r(0).toDouble, \nr(1).toDouble, r(2).toDouble, r(3).toDouble))).cache()\n//pca takes a datamatrix, create it\nval mat: RowMatrix = new RowMatrix(parsedData)\n// Compute the top 4 principal components.\nval pc: Matrix = mat.computePrincipalComponents(4) \n// Principal components are stored in a local dense matrix.\nprintln(pc)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-0.36158967738144887  -0.6565398832858419  0.5809972798275919    0.3172545471685654    \n0.08226888989221673   -0.729712371326486   -0.5964180879380797   -0.324094352418032    \n-0.8565721052905274   0.17576740342866543  -0.07252407548692685  -0.47971898732994167  \n-0.35884392624821654  0.07470647013501308  -0.5490609107266611   0.7511205603807821    \n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 76, "cell_type": "code", "source": "//Computes the covariance matrix, treating each row as an observation.\nval mat: RowMatrix = new RowMatrix(parsedData)\nval pc: Matrix = mat.computeCovariance() \nprintln(pc)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.6856935123042476     -0.039268456375836536  1.2736823266219162   0.5169038031319939    \n-0.039268456375836536  0.18800402684563267    -0.3217127516778646  -0.11798120805369283  \n1.2736823266219197     -0.3217127516778646    3.113179418344508    1.2963874720357937    \n0.5169038031319948     -0.11798120805369283   1.2963874720357937   0.5824143176733789    \n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##SVD", "cell_type": "markdown", "metadata": {}}, {"execution_count": 86, "cell_type": "code", "source": "import org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.SingularValueDecomposition\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.rdd.RDD\n\nval iris = sc.textFile(\"hdfs://10.122.48.12:8020/data/iris.csv\")\n//parse csv to vectors\nval data_iris = iris.map(l => l.split(\",\",-1) )\nval parsedData = data_iris.map(r => Vectors.dense(Array( r(0).toDouble, \nr(1).toDouble, r(2).toDouble, r(3).toDouble))).cache()\n//pca takes a datamatrix, create it\nval mat: RowMatrix = new RowMatrix(parsedData)\n\n// Compute the top 4 singular values and corresponding singular vectors.\nval svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(4, computeU = true)\nval U: RowMatrix = svd.U  // The U factor is a RowMatrix.\nval V: Matrix = svd.V   //   The V factor is a local dense matrix.\nprintln(V)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-0.7511680505936613   -0.2858309594911222  0.49942378035010954  0.3234549582005334    \n-0.3797883666928143   -0.5448897554611347  -0.6750249882865683  -0.3212432351148067   \n-0.5131509372098667   0.7088987448097407   -0.0547198252265472  -0.48077481836613056  \n-0.16787933742053857  0.3447584467378048   -0.5402988927775985  0.7490228620899887    \n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##Linear Regression Least Squares with SGD", "cell_type": "markdown", "metadata": {}}, {"execution_count": 88, "cell_type": "code", "source": "//build a simple linear model to predict label values\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\n\n//load and parse data\nval iris = sc.textFile(\"hdfs://10.122.48.12:8020/data/iris.csv\")\nval parsedData = iris.map { line =>\n  val parts = line.split(',')\n  LabeledPoint(parts(0).toDouble, Vectors.dense(parts.dropRight(1).map(_.toDouble)))\n}.cache()\n\n// Building the model\nval numIterations = 100\nval model = LinearRegressionWithSGD.train(parsedData, numIterations)\n\n// Evaluate model on training examples and compute training error\nval valuesAndPreds = parsedData.map { point =>\n  val prediction = model.predict(point.features)\n  (point.label, prediction)\n}\nval MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()\nprintln(\"Training Mean Squared Error = \" + MSE)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Training Mean Squared Error = 2.9717406458599726E256\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Scala (Nitro Spark)", "name": "nitro_spark", "language": "scala"}, "language_info": {"name": "scala"}}}