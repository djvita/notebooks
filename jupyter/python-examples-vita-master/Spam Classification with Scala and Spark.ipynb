{"nbformat_minor": 0, "cells": [{"source": "# Tutorial - Binary Classification with Spark and Scala\nThis notebook shows how you can perform [binary classification](http://en.wikipedia.org/wiki/Binary_classification) using IBM Knowledge Anyhow Workbench and Apache Spark. In particular, this notebook shows how you can solve a common machine learning problem: spam classification. This notebook uses a small dataset to highlight the usage of Spark but it can scale to use big data without changing any code.\n\nThis notebook will cover the following steps:\n\n1. Connect to a Spark cluster\n2. Load training and test datasets\n3. Train the machine learning model\n4. Test the machine learning model\n\n## Background\n***Logistic Regression***\n\nA [Logistic Regression model](http://en.wikipedia.org/wiki/Logistic_regression) is used to classify items into one of two possible categories (binary classification). The model is a linear separating plane which is used to identify the classification of items in the set. All items on one side of the linear separating plane belong to one category and all items on the other side of the linear separating plane belong to the other category.\n\nThere are several ways to train a Logistic Regression model. This notebook uses a [Stochastic Gradient Descent (SGD)](http://en.wikipedia.org/wiki/Stochastic_gradient_descent) implementation because [Spark MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) includes an implementation of SGD in its library. SGD is an appropriate implementation for large-scale machine learning problems so, while this notebook does not use a big dataset, it's an appropriate example for Spark.\n\n\n***Dataset***\n\nThis notebook uses the [Spambase dataset](https://archive.ics.uci.edu/ml/datasets/Spambase) which was created by Mark Hopkins, Erik Reeber, George Forman, and Jaap Suermondt at HP Labs. It includes 4601 observations corresponding to email messages, 1813 of which are spam. The spambase data has 57 real valued explanatory variables which characterize the contents of an email and one binary response variable indicating if the email is spam. Of the 57 explanatory variables, 48 describe word frequency, 6 describe character frequency, and 3 describe sequences of capital letters. \nThe dataset may be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/).", "cell_type": "markdown", "metadata": {}}, {"source": "##Connect to a Spark Cluster", "cell_type": "markdown", "metadata": {}}, {"source": "First, instruct a provisioner to create a Spark Kernel on the Spark cluster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": "nitro.spark.create_kernel(\"http://169.53.153.8:9000/\")", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "When the code above reports \"Kernel ready\", your notebook is now connected to a Scala kernel on your Spark cluster. Commands you issue from here on out execute in that kernel. Now, tell the kernel to import the required libraries.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import org.apache.spark.SparkContext\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.util.MLUtils", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Load training and test datasets", "cell_type": "markdown", "metadata": {}}, {"source": "The dataset is stored on the Spark cluster. Load the dataset from HDFS on the Spark cluster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "val data = sc.textFile(\"hdfs://10.122.48.12:8020/data/email-data.csv\")", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "The dataset includes 4,601 records which have already been [featurized](http://en.wikipedia.org/wiki/Feature_%28machine_learning%29). Each record contains 57 feature scores (characteristics of the email) followed by a classification of the email (spam or not spam).", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "println(\"Total records in dataset: \" + data.count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "\r                                                                                \rTotal records in dataset: 4601\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 5, "cell_type": "code", "source": "// record format: <feature_1>;<feature_2>;<feature_3>;...<feature_57>;<classification>\nprintln(\"Sample record:\")\ndata.take(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Sample record:\n"}, {"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "Array(0;0.64;0.64;0;0.32;0;0;0;0;0;0;0.64;0;0;0;0.32;0;1.29;1.93;0;0.96;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0.778;0;0;3.756;61;278;1)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "***Transform the dataset***\n\nSpark MLlib provides a machine learning API which can work with data represeted in [LabeledPoints](https://spark.apache.org/docs/0.8.1/api/mllib/org/apache/spark/mllib/regression/LabeledPoint.html). `LabeledPoint` represents data in the following format:\n\n```<classification-label>,<vector-of-features>```\n\nTransform the dataset using LabeledPoints.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "// LabeledPoint constructor expects (<label>,<feature_vector>) so parse appropriately\nval parsedData = data.map { line =>\n  val parts = line.split(';')\n  // Label is in the 57th position (0-indexed). Everything but the right-most item is a feature \n  LabeledPoint(parts(57).toDouble, Vectors.dense(parts.dropRight(1).map(_.toDouble)))\n}.cache()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Each transformed record now looks like this:\n\n```<label>,[<feature_1>,<feature_2>,<feature_3>,...<feature_57>]```", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "println(\"Sample of record transformed using LabeledPoint:\")\nparsedData.take(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Sample of record transformed using LabeledPoint:\n"}, {"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "Array((1.0,[0.0,0.64,0.64,0.0,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.64,0.0,0.0,0.0,0.32,0.0,1.29,1.93,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.778,0.0,0.0,3.756,61.0,278.0]))"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 9, "cell_type": "code", "source": "println(\"Total records in transformed dataset: \" + parsedData.count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "\r                                                                                \rTotal records in transformed dataset: 4601\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Machine learning models rely on training sets from which the function for calculating the linear separating plane is derived. Additonally, machine learning models rely on test sets which can be used to assess the performance of the model. Generate the training and test sets from the original dataset.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "// Split data into training (80%) and test (20%).\nval splits = parsedData.randomSplit(Array(0.8, 0.2), seed = 11L)\nval training = splits(0).cache()\nval test = splits(1)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 11, "cell_type": "code", "source": "println(\"Total records in training set: \" + training.count())\nprintln(\"Total records in test set: \" + test.count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Total records in training set: 3616\nTotal records in test set: 985\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Train the machine learning model\n\nThe Spark cluster is a shared environment. The training of the model (which you'll run in the next code cell) is the most resource-intensive step in this notebook. You may experience a delay in the completion of the training step. If the training step does not complete within a reasonable time (<10 mins) then you should try to restart the kernel.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "// Train the model using the MLlib API\nval numIterations = 500\nval model = LogisticRegressionWithSGD.train(training, numIterations)", "outputs": [{"output_type": "stream", "name": "stdout", "text": ""}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Test the machine learning model\n\nAt this point, you have a Logistic Regression model trained to classify featurized emails as spam or not spam. It is important to test the model using a set of emails with known classifications.\n\nUse the model's predict function to test it against the test set which you created above. Store the predicted classification of each test record in a list that maps to the known classification of each test record so we can easily compute the performance of the model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "// Create an RDD of predicted and known classifications for the test set.\nval predictedAndKnown = test.map { record =>\n  val predicted = model.predict(record.features)\n  (predicted, record.label)\n}", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Create an array from the [RDD](http://spark.apache.org/docs/1.2.1/api/scala/index.html#org.apache.spark.rdd.RDD) for easy iteration.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "var predictedAndKnown_arr = predictedAndKnown.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "\r                                                                                \r"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Print the first 5 predicted and known classifications.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "for( i <- 0 to 4 ){\n   println(predictedAndKnown_arr(i))\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(1.0,1.0)\n(1.0,1.0)\n(0.0,1.0)\n(1.0,1.0)\n(1.0,1.0)\n"}], "metadata": {"scrolled": true, "collapsed": false, "trusted": true}}, {"source": "Iterate through the array to compare the predicted classification against the known classification. Keep a count of the correct classifications to determine how well the model performs.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": "var correctCount = 0\npredictedAndKnown_arr.foreach{i =>\n    if(i._1 == i._2){ //If this is true then model predicted correctly\n      correctCount = correctCount + 1;\n    }\n}", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 29, "cell_type": "code", "source": "println(\"Correct predictions: \" + correctCount)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Correct predictions: 695\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "The [quality of a model](http://en.wikipedia.org/wiki/Precision_and_recall) is often measured by accuracy. Accuracy is the percentage of correct predictions made by a model (correct predictions / total predictions). Calculate the accuracy of your model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "println(\"Model Accuracy: \" + correctCount.toFloat/test.count().toFloat)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Model Accuracy: 0.70558375\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Conclusion\nIn this notebook, you created a binary classification model using a Spark cluster. You used core Spark functionality and the Spark MLlib library to:\n* load and transform pre-existing data\n* train a machine learning model\n* test a machine learning model", "cell_type": "markdown", "metadata": {}}, {"source": "## References\n1. [Machine Learning Repository: Spambase Dataset](https://archive.ics.uci.edu/ml/datasets/Spambase) \n2. [Learning Spark, Lightning Fast Data Analysis](http://shop.oreilly.com/product/0636920028512.do) by Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia; O'Reilly Media\n3. [Spark MLlib Logistic Regression Doc](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression)\n\n***Additional Spark/MLlib Resources***\n* [Main Page](https://spark.apache.org/docs/latest/)\n* [Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)\n* [MlLib](https://spark.apache.org/docs/latest/mllib-guide.html)\n* [Logistic Regression](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Scala (Nitro Spark)", "name": "nitro_spark", "language": "scala"}, "language_info": {"name": "scala"}}}